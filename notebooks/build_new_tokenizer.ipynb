{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build/Train a New Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [HuggingFace video: build](https://www.youtube.com/watch?v=MR8tZm5ViWU)\n",
    "* [HuggingFace video: train](https://www.youtube.com/watch?v=DJimQynXZsQ)\n",
    "* [HuggingFace course](https://huggingface.co/learn/nlp-course/chapter6/8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenisation Steps: \n",
    "\n",
    "1. Normalisation\n",
    "2. Pre-tokenisation\n",
    "3. Model\n",
    "4. Post-processing\n",
    "5. Decoding\n",
    "\n",
    "Build-your-own Steps:\n",
    "\n",
    "1. Gather a corpus\n",
    "2. Create a `backend_tokenizer` with HF (steps 1-4 in tokenisation)\n",
    "3. Load `backend_tokenizer` in a HF transformers tokenizer\n",
    "\n",
    "Why to Train your own: \n",
    "\n",
    "* Tokenizer won't be suitable if trained on a non-similar corpus to your purpose\n",
    "* Different language\n",
    "* New characters (e.g. accents!)\n",
    "* New domain\n",
    "* New style (e.g. from another century)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets.dataset_dict import DatasetDict\n",
    "from tokenizers import (decoders,\n",
    "                        models, \n",
    "                        normalizers, \n",
    "                        pre_tokenizers, \n",
    "                        processors,\n",
    "                        Regex,\n",
    "                        trainers, \n",
    "                        Tokenizer, \n",
    ")\n",
    "from transformers import AutoTokenizer, BertTokenizerFast, PreTrainedTokenizerFast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer Failure Modes, Examples \n",
    "\n",
    "From: [Hugging Face, Train Your Own](https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/videos/train_new_tokenizer.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\n",
    "  'huggingface-course/bert-base-uncased-tokenizer-without-normalizer'\n",
    ")\n",
    "text = \"here is a sentence adapted to our tokenizer\"\n",
    "print(text)\n",
    "print(tokenizer.tokenize(text))\n",
    "print(\"The base BERT tokenizer does well, on 'simple' English\", end=\"\\n\\n\")\n",
    "\n",
    "\n",
    "text = \"এই বাক্যটি আমাদের টোকেনাইজারের উপযুক্ত নয়\"\n",
    "print(text)\n",
    "print(tokenizer.tokenize(text))\n",
    "print(\"It struggles on Bengali: splitting one word into _many_ tokens, or it does not recognize words at all; [UNK]\", end=\"\\n\\n\")\n",
    "\n",
    "\n",
    "text = \"this tokenizer does not know àccënts and CAPITAL LETTERS\"\n",
    "print(text)\n",
    "print(tokenizer.tokenize(text))\n",
    "print(\"It loses accents, capitals. problematic for other languages and proper nouns\", end=\"\\n\\n\")\n",
    "\n",
    "text = \"the medical vocabulary is divided into many sub-token: paracetamol, phrayngitis\"\n",
    "print(text)\n",
    "print(tokenizer.tokenize(text))\n",
    "print(\"It is missing critical vocabulary for domain-usage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Excessive splitting is problematic: models are sequence-length-limited; common words may carry additional semantic value\n",
    "* `[UNK]` strips all information from the token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare = load_dataset(\"tiny_shakespeare\")\n",
    "train_split = shakespeare[\"train\"]\n",
    "test_split = shakespeare[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\n",
    "\n",
    "tokenizer.normalizer = normalizers.Sequence([\n",
    "    normalizers.Replace(Regex(r\"[\\p{Other}&&[^\\n\\t\\r]]\"), \"\"),    # cleanup control \n",
    "    normalizers.Replace(Regex(r\"[\\s]\"), \" \"),     # characters not visibile in text\n",
    "    normalizers.Lowercase(),\n",
    "    normalizers.NFD(),     # NFD Unicode Normalizer\n",
    "    normalizers.StripAccents() # remove accents\n",
    "])\n",
    "\n",
    "# what's the normalizer doing? \n",
    "print(\"Normalizer: Héllò hôw are ü? --> \", tokenizer.normalizer.normalize_str(\"Héllò hôw are ü?\"))\n",
    "\n",
    "# chain two pre-tokenizers \n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Sequence([pre_tokenizers.WhitespaceSplit(),   #  pre-tokenizer splits on whitespace and all characters that are not letters, digits, or the underscore character, so it technically splits on whitespace and punctuation\n",
    "                                                   pre_tokenizers.Punctuation()])     # isolate punctiation \n",
    "\n",
    "\n",
    "# what's the pre-tokenizer doing?\n",
    "print(\"Pre-tokenizer: Let's test my pre-tokenizer. -->\", tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test my pre-tokenizer.\"))\n",
    "\n",
    "\n",
    "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "\n",
    "trainer = trainers.WordPieceTrainer(vocab_size=10000, \n",
    "                                    special_tokens=special_tokens,\n",
    "                                    continuing_subword_prefix=\"##\",    # can be any characters... but we don't want something that would occur \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_corpus(dataset: DatasetDict):\n",
    "    for i in range(0, len(dataset), 1000):\n",
    "        yield dataset[i: i+1000][\"text\"]\n",
    "\n",
    "tokenizer.train_from_iterator(get_training_corpus(train_split), trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have to specify how to treat a single sentence and a pair of sentences. For both, we write the special tokens we want to use; the first (or single) sentence is represented by $A, while the second sentence (if encoding a pair) is represented by $B. For each of these (special tokens and sentences), we also specify the corresponding token type ID after a colon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve ids of special tokens, needed for post-processing of sequences\n",
    "cls_token_id = tokenizer.token_to_id(\"[CLS]\")\n",
    "sep_token_id = tokenizer.token_to_id(\"[SEP]\")\n",
    "\n",
    "# set beginning of each sequence to have CLS; SEP to end of each sequence and before a new sequence\n",
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=f\"[CLS]:0 $A:0 [SEP]:0\",\n",
    "    pair=f\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n",
    "    special_tokens=[(\"[CLS]\", cls_token_id), (\"[SEP]\", sep_token_id)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, define the decoder, allowing removal of hashtags. Notice how true re-normalisation never occurs: we cannot go back to accent characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decoder = decoders.WordPiece(prefix=\"##\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"../data/06_models/bert-remake-tokenizer.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load it into a FastTokenizer from Transformers library\n",
    "\n",
    "Two options: \n",
    "\n",
    "1. PretrainedTokenizerFast\n",
    "2. BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_tokenizer = BertTokenizerFast(\n",
    "    #tokenizer_object=tokenizer,\n",
    "    tokenizer_file=\"../data/06_models/bert-remake-tokenizer.json\",\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"To die, - To sleep, - To sleep!\"\\\n",
    "    \"Perchance to dream: - ay, there's the rub;\"\\\n",
    "    \"For in that sleep of death what dreams may come,\"\\\n",
    "    \"When we have shuffled off this mortal coil,\"\\\n",
    "    \"Must give us pause: there's the respect\"\\\n",
    "    \"That makes calamity of so long life;\"\n",
    "\n",
    "encoded = wrapped_tokenizer.encode(s, padding=True, add_special_tokens=True)\n",
    "print(encoded)\n",
    "encoded_plus = wrapped_tokenizer.encode_plus(s, padding=False, add_special_tokens=True)\n",
    "\n",
    "# print attention mask // no attention mask, single sequence\n",
    "# for a in encoded_plus['attention_mask']:\n",
    "#     print(a)\n",
    "\n",
    "for a in encoded_plus['input_ids']:\n",
    "    print(wrapped_tokenizer.convert_ids_to_tokens(a), end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train AutoTokenizer\n",
    "\n",
    "All HuggingFace Autotokenizers can `train_new_from_iterator()`. \n",
    "\n",
    "This uses a known architecture (e.g. BERT-WordPiece), and creates from a new corprus a vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_tokenizer.train_new_from_iterator(\n",
    "    get_training_corpus(train_split),\n",
    "    vocab_size=25000,\n",
    "    new_special_tokens=None,\n",
    "    special_tokens_map=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_tokenizer.save_pretrained(\"../data/06_models/auto_tokenizer_retrained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tok",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
