{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build/Train a New Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [HuggingFace video: build](https://www.youtube.com/watch?v=MR8tZm5ViWU)\n",
    "* [HuggingFace video: train](https://www.youtube.com/watch?v=DJimQynXZsQ)\n",
    "* [HuggingFace course](https://huggingface.co/learn/nlp-course/chapter6/8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenisation Steps: \n",
    "\n",
    "1. Normalisation\n",
    "2. Pre-tokenisation\n",
    "3. Model\n",
    "4. Post-processing\n",
    "5. Decoding\n",
    "\n",
    "Build-your-own Steps:\n",
    "\n",
    "1. Gather a corpus\n",
    "2. Create a `backend_tokenizer` with HF (steps 1-4 in tokenisation)\n",
    "3. Load `backend_tokenizer` in a HF transformers tokenizer\n",
    "\n",
    "Why to Train your own: \n",
    "\n",
    "* Tokenizer won't be suitable if trained on a non-similar corpus to your purpose\n",
    "* Different language\n",
    "* New characters (e.g. accents!)\n",
    "* New domain\n",
    "* New style (e.g. from another century)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bcech\\anaconda3\\envs\\tok\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets.dataset_dict import DatasetDict\n",
    "from tokenizers import (decoders,\n",
    "                        models, \n",
    "                        normalizers, \n",
    "                        pre_tokenizers, \n",
    "                        processors,\n",
    "                        Regex,\n",
    "                        trainers, \n",
    "                        Tokenizer, \n",
    ")\n",
    "from transformers import AutoTokenizer, BertTokenizerFast, PreTrainedTokenizerFast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer Failure Modes, Examples \n",
    "\n",
    "From: [Hugging Face, Train Your Own](https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/videos/train_new_tokenizer.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['here', 'is', 'a', 'sentence', 'adapted', 'to', 'our', 'token', '##izer']\n",
      "The base BERT tokenizer does well, on 'simple' English\n",
      "\n",
      "['এ', '##ই', '[UNK]', 'আ', '##ম', '##া', '##দ', '##ে', '##র', '[UNK]', '[UNK]', '[UNK]']\n",
      "It struggles on Bengali: splitting one word into _many_ tokens, or it does not recognize words at all; [UNK]\n",
      "\n",
      "['this', 'token', '##izer', 'does', 'not', 'know', '[UNK]', 'and', '[UNK]', '[UNK]']\n",
      "It loses accents, capitals. problematic for other languages and proper nouns\n",
      "\n",
      "['the', 'medical', 'vocabulary', 'is', 'divided', 'into', 'many', 'sub', '-', 'token', ':', 'para', '##ce', '##tam', '##ol', ',', 'ph', '##ray', '##ng', '##itis']\n",
      "It is missing critical vocabulary for domain-usage\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\n",
    "  'huggingface-course/bert-base-uncased-tokenizer-without-normalizer'\n",
    ")\n",
    "text = \"here is a sentence adapted to our tokenizer\"\n",
    "print(tokenizer.tokenize(text))\n",
    "print(\"The base BERT tokenizer does well, on 'simple' English\", end=\"\\n\\n\")\n",
    "\n",
    "\n",
    "text = \"এই বাক্যটি আমাদের টোকেনাইজারের উপযুক্ত নয়\"\n",
    "print(tokenizer.tokenize(text))\n",
    "print(\"It struggles on Bengali: splitting one word into _many_ tokens, or it does not recognize words at all; [UNK]\", end=\"\\n\\n\")\n",
    "\n",
    "\n",
    "text = \"this tokenizer does not know àccënts and CAPITAL LETTERS\"\n",
    "print(tokenizer.tokenize(text))\n",
    "print(\"It loses accents, capitals. problematic for other languages and proper nouns\", end=\"\\n\\n\")\n",
    "\n",
    "text = \"the medical vocabulary is divided into many sub-token: paracetamol, phrayngitis\"\n",
    "print(tokenizer.tokenize(text))\n",
    "print(\"It is missing critical vocabulary for domain-usage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Excessive splitting is problematic: models are sequence-length-limited; common words may carry additional semantic value\n",
    "* `[UNK]` strips all information from the token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset tiny_shakespeare (C:/Users/bcech/.cache/huggingface/datasets/tiny_shakespeare/default/1.0.0/b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e)\n",
      "100%|██████████| 3/3 [00:00<00:00, 184.58it/s]\n"
     ]
    }
   ],
   "source": [
    "shakespeare = load_dataset(\"tiny_shakespeare\")\n",
    "train_split = shakespeare[\"train\"]\n",
    "test_split = shakespeare[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizer: Héllò hôw are ü? -->  hello how are u?\n",
      "Pre-tokenizer: Let's test my pre-tokenizer. --> [('Let', (0, 3)), (\"'\", (3, 4)), ('s', (4, 5)), ('test', (6, 10)), ('my', (11, 13)), ('pre', (14, 17)), ('-', (17, 18)), ('tokenizer', (18, 27)), ('.', (27, 28))]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\n",
    "\n",
    "tokenizer.normalizer = normalizers.Sequence([\n",
    "    normalizers.Replace(Regex(r\"[\\p{Other}&&[^\\n\\t\\r]]\"), \"\"),    # cleanup control \n",
    "    normalizers.Replace(Regex(r\"[\\s]\"), \" \"),     # characters not visibile in text\n",
    "    normalizers.Lowercase(),\n",
    "    normalizers.NFD(),     # NFD Unicode Normalizer\n",
    "    normalizers.StripAccents() # remove accents\n",
    "])\n",
    "\n",
    "# what's the normalizer doing? \n",
    "print(\"Normalizer: Héllò hôw are ü? --> \", tokenizer.normalizer.normalize_str(\"Héllò hôw are ü?\"))\n",
    "\n",
    "# chain two pre-tokenizers \n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Sequence([pre_tokenizers.WhitespaceSplit(),   #  pre-tokenizer splits on whitespace and all characters that are not letters, digits, or the underscore character, so it technically splits on whitespace and punctuation\n",
    "                                                   pre_tokenizers.Punctuation()])     # isolate punctiation \n",
    "\n",
    "\n",
    "# what's the pre-tokenizer doing?\n",
    "print(\"Pre-tokenizer: Let's test my pre-tokenizer. -->\", tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test my pre-tokenizer.\"))\n",
    "\n",
    "\n",
    "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "\n",
    "trainer = trainers.WordPieceTrainer(vocab_size=10000, \n",
    "                                    special_tokens=special_tokens,\n",
    "                                    continuing_subword_prefix=\"##\",    # can be any characters... but we don't want something that would occur \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_corpus(dataset: DatasetDict):\n",
    "    for i in range(0, len(dataset), 1000):\n",
    "        yield dataset[i: i+1000][\"text\"]\n",
    "\n",
    "tokenizer.train_from_iterator(get_training_corpus(train_split), trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have to specify how to treat a single sentence and a pair of sentences. For both, we write the special tokens we want to use; the first (or single) sentence is represented by $A, while the second sentence (if encoding a pair) is represented by $B. For each of these (special tokens and sentences), we also specify the corresponding token type ID after a colon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve ids of special tokens, needed for post-processing of sequences\n",
    "cls_token_id = tokenizer.token_to_id(\"[CLS]\")\n",
    "sep_token_id = tokenizer.token_to_id(\"[SEP]\")\n",
    "\n",
    "# set beginning of each sequence to have CLS; SEP to end of each sequence and before a new sequence\n",
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=f\"[CLS]:0 $A:0 [SEP]:0\",\n",
    "    pair=f\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n",
    "    special_tokens=[(\"[CLS]\", cls_token_id), (\"[SEP]\", sep_token_id)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, define the decoder, allowing removal of hashtags. Notice how true re-normalisation never occurs: we cannot go back to accent characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decoder = decoders.WordPiece(prefix=\"##\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"../data/06_models/bert-remake-tokenizer.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load it into a FastTokenizer from Transformers library\n",
    "\n",
    "Two options: \n",
    "\n",
    "1. PretrainedTokenizerFast\n",
    "2. BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_tokenizer = BertTokenizerFast(\n",
    "    #tokenizer_object=tokenizer,\n",
    "    tokenizer_file=\"../data/06_models/bert-remake-tokenizer.json\",\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 80, 647, 9, 10, 80, 1053, 9, 10, 80, 1053, 5, 4570, 80, 1094, 13, 10, 543, 9, 225, 8, 34, 71, 8426, 14, 112, 100, 107, 1053, 88, 350, 162, 2270, 291, 239, 9, 296, 128, 146, 108, 2306, 775, 417, 140, 1693, 562, 235, 9, 332, 382, 292, 4075, 13, 225, 8, 34, 71, 1719, 83, 75, 898, 5258, 88, 143, 623, 448, 14, 3]\n",
      "[CLS] to die , - to sleep , - to sleep ! perchance to dream : - ay , there ' s the rub ; for in that sleep of death what dreams may come , when we have sh ##uff ##led off this mortal co ##il , must give us pause : there ' s the respect ##th ##at makes calamity of so long life ; [SEP] "
     ]
    }
   ],
   "source": [
    "s = \"To die, - To sleep, - To sleep!\"\\\n",
    "    \"Perchance to dream: - ay, there's the rub;\"\\\n",
    "    \"For in that sleep of death what dreams may come,\"\\\n",
    "    \"When we have shuffled off this mortal coil,\"\\\n",
    "    \"Must give us pause: there's the respect\"\\\n",
    "    \"That makes calamity of so long life;\"\n",
    "\n",
    "encoded = wrapped_tokenizer.encode(s, padding=True, add_special_tokens=True)\n",
    "print(encoded)\n",
    "encoded_plus = wrapped_tokenizer.encode_plus(s, padding=False, add_special_tokens=True)\n",
    "\n",
    "# print attention mask // no attention mask, single sequence\n",
    "# for a in encoded_plus['attention_mask']:\n",
    "#     print(a)\n",
    "\n",
    "for a in encoded_plus['input_ids']:\n",
    "    print(wrapped_tokenizer.convert_ids_to_tokens(a), end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train AutoTokenizer\n",
    "\n",
    "All HuggingFace Autotokenizers can `train_new_from_iterator()`. \n",
    "\n",
    "This uses a known architecture (e.g. BERT-WordPiece), and creates from a new corprus a vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='bert-base-cased', vocab_size=18194, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_tokenizer.train_new_from_iterator(\n",
    "    get_training_corpus(train_split),\n",
    "    vocab_size=25000,\n",
    "    new_special_tokens=None,\n",
    "    special_tokens_map=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../data/06_models/auto_tokenizer_retrained\\\\tokenizer_config.json',\n",
       " '../data/06_models/auto_tokenizer_retrained\\\\special_tokens_map.json',\n",
       " '../data/06_models/auto_tokenizer_retrained\\\\vocab.txt',\n",
       " '../data/06_models/auto_tokenizer_retrained\\\\added_tokens.json',\n",
       " '../data/06_models/auto_tokenizer_retrained\\\\tokenizer.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_tokenizer.save_pretrained(\"../data/06_models/auto_tokenizer_retrained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tok",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
