{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sub-word-level Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall flaws of \n",
    "1. Word-based approaches\n",
    "\n",
    "* Very large vocabularies\n",
    "* Large quantity of out-of-vocabulary tokens\n",
    "* Loss of meaning across very similar words (dog, dogs, sun, sunny, hap, happy)\n",
    "\n",
    "2. Character-based approaches\n",
    "\n",
    "* Very long sequences\n",
    "* Less meaningful individual tokens\n",
    "\n",
    "So,\n",
    "\n",
    "A middle-ground between the two is found.\n",
    "\n",
    "1. Frequently used words should not be split (into subwords)\n",
    "2. Rare words should be split into \"meaningful\" subwords\n",
    "\n",
    "`dog` -> `dog`\n",
    "`dogs` -> `dog` `s`\n",
    "\n",
    "see [hf video on it](https://www.youtube.com/watch?v=zHvTiHr506c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good example given is `tokenization`: \n",
    "\n",
    "`token` -- `ization`  \n",
    "\n",
    "`token`: Token, tokens, tokenizing, tokenization, tokenized, tokenizes, tokenizable, tokenizability  \n",
    "`ization`: tokenization, modernization, globalization, industrialization, organization, realization,   utilization, ...  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sub-word-based tokenizers typically identify when a word is split, by special character padding: \n",
    "\n",
    "`token` `##ization` (BERT)\n",
    "\n",
    "The number of `#` may or may not refer to the count of preceding letters it was split from (in this case it doesn't)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sub-Word Algorithms\n",
    "\n",
    "1. WordPiece (BERT, DistilBERT,  ...)\n",
    "2. Unigram (XLNET, ALBERT)\n",
    "3. Byte-Pair Encoding (RoBERTa, GPT-2+, T5, ...)\n",
    "3. [SentencePiece](https://huggingface.co/docs/transformers/tokenizer_summary#sentencepiece+) (sorta T5, ...) ((all that use it also use Unigram))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordPiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall strategy is to start with a very large vocabulary, and iteratively shrink it. Each iteration a unigram loss is calculated and the bottom _p_ tokens are reduced.\n",
    "\n",
    "* Unigram model is stats model: $$P(t1, t2, t3, ... tN)$$\n",
    "\n",
    "* Unigram model assumes that the occurence of each word is independent of its previous word. Thus, can calulate overall probability of next word: \n",
    "\n",
    "$$P(t1, t2, t3, ... tN)= P(t1) x P(t2) x P(t3) x ... x P(TN)$$\n",
    "\n",
    "i.e. the probability of a text is the probability of the tokens that compose it. This means that it can't perform meaningful text generation: it will always predict the single highest probability token. So, what's it good for? \n",
    "\n",
    "It is a useful model to estimate the relative likelihood of different phrases. (..?)  \n",
    "\n",
    "**TODO Fill iterations**\n",
    "\n",
    "* each iteration, calculate probabilities of each token \n",
    "* remove one that impacts loss the least && is not an elementary token (as this would prevent ever spelling a word that contains it)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Byte-Pair Encoding (BPE) Tokenizaiton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea is originally text compression algorithm. \n",
    "\n",
    "Psuedo-algorithm: \n",
    "\n",
    "* Take a corpus, split into all words, then into all characters\n",
    "* count each pair frequency, finding most common pair(s)\n",
    "* take most common pair as a token and add to vocabulary\n",
    "* iterate until desired vocab size is reached "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [HuggingFace Video](https://www.youtube.com/watch?v=HEikzVL-lZU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SentencePiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[02/12/24 01:07:30] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Resolved project path as: c:\\Users\\bcech\\Documents\\ML\\tolkenizers.     <a href=\"file://c:\\Users\\bcech\\anaconda3\\envs\\tok\\Lib\\site-packages\\kedro\\ipython\\__init__.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">__init__.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://c:\\Users\\bcech\\anaconda3\\envs\\tok\\Lib\\site-packages\\kedro\\ipython\\__init__.py#149\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">149</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         To set a different path, run <span style=\"color: #008000; text-decoration-color: #008000\">'%reload_kedro &lt;project_root&gt;'</span>            <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[02/12/24 01:07:30]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Resolved project path as: c:\\Users\\bcech\\Documents\\ML\\tolkenizers.     \u001b]8;id=305817;file://c:\\Users\\bcech\\anaconda3\\envs\\tok\\Lib\\site-packages\\kedro\\ipython\\__init__.py\u001b\\\u001b[2m__init__.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=79656;file://c:\\Users\\bcech\\anaconda3\\envs\\tok\\Lib\\site-packages\\kedro\\ipython\\__init__.py#149\u001b\\\u001b[2m149\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         To set a different path, run \u001b[32m'%reload_kedro \u001b[0m\u001b[32m<\u001b[0m\u001b[32mproject_root\u001b[0m\u001b[32m>\u001b[0m\u001b[32m'\u001b[0m            \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[02/12/24 01:07:31] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Kedro project tolkenizers                                              <a href=\"file://c:\\Users\\bcech\\anaconda3\\envs\\tok\\Lib\\site-packages\\kedro\\ipython\\__init__.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">__init__.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://c:\\Users\\bcech\\anaconda3\\envs\\tok\\Lib\\site-packages\\kedro\\ipython\\__init__.py#119\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">119</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[02/12/24 01:07:31]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Kedro project tolkenizers                                              \u001b]8;id=891713;file://c:\\Users\\bcech\\anaconda3\\envs\\tok\\Lib\\site-packages\\kedro\\ipython\\__init__.py\u001b\\\u001b[2m__init__.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=241776;file://c:\\Users\\bcech\\anaconda3\\envs\\tok\\Lib\\site-packages\\kedro\\ipython\\__init__.py#119\u001b\\\u001b[2m119\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Defined global variable <span style=\"color: #008000; text-decoration-color: #008000\">'context'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'session'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'catalog'</span> and            <a href=\"file://c:\\Users\\bcech\\anaconda3\\envs\\tok\\Lib\\site-packages\\kedro\\ipython\\__init__.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">__init__.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://c:\\Users\\bcech\\anaconda3\\envs\\tok\\Lib\\site-packages\\kedro\\ipython\\__init__.py#120\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">120</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">'pipelines'</span>                                                            <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Defined global variable \u001b[32m'context'\u001b[0m, \u001b[32m'session'\u001b[0m, \u001b[32m'catalog'\u001b[0m and            \u001b]8;id=946696;file://c:\\Users\\bcech\\anaconda3\\envs\\tok\\Lib\\site-packages\\kedro\\ipython\\__init__.py\u001b\\\u001b[2m__init__.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=371796;file://c:\\Users\\bcech\\anaconda3\\envs\\tok\\Lib\\site-packages\\kedro\\ipython\\__init__.py#120\u001b\\\u001b[2m120\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32m'pipelines'\u001b[0m                                                            \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[02/12/24 01:07:33] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Registered line magic <span style=\"color: #008000; text-decoration-color: #008000\">'run_viz'</span>                                        <a href=\"file://c:\\Users\\bcech\\anaconda3\\envs\\tok\\Lib\\site-packages\\kedro\\ipython\\__init__.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">__init__.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://c:\\Users\\bcech\\anaconda3\\envs\\tok\\Lib\\site-packages\\kedro\\ipython\\__init__.py#126\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">126</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[02/12/24 01:07:33]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Registered line magic \u001b[32m'run_viz'\u001b[0m                                        \u001b]8;id=946367;file://c:\\Users\\bcech\\anaconda3\\envs\\tok\\Lib\\site-packages\\kedro\\ipython\\__init__.py\u001b\\\u001b[2m__init__.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=548497;file://c:\\Users\\bcech\\anaconda3\\envs\\tok\\Lib\\site-packages\\kedro\\ipython\\__init__.py#126\u001b\\\u001b[2m126\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Resolved project path as: C:\\Users\\bcech\\Documents\\ML\\tolkenizers.     <a href=\"file://c:\\Users\\bcech\\anaconda3\\envs\\tok\\Lib\\site-packages\\kedro\\ipython\\__init__.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">__init__.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://c:\\Users\\bcech\\anaconda3\\envs\\tok\\Lib\\site-packages\\kedro\\ipython\\__init__.py#149\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">149</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         To set a different path, run <span style=\"color: #008000; text-decoration-color: #008000\">'%reload_kedro &lt;project_root&gt;'</span>            <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Resolved project path as: C:\\Users\\bcech\\Documents\\ML\\tolkenizers.     \u001b]8;id=93606;file://c:\\Users\\bcech\\anaconda3\\envs\\tok\\Lib\\site-packages\\kedro\\ipython\\__init__.py\u001b\\\u001b[2m__init__.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=782038;file://c:\\Users\\bcech\\anaconda3\\envs\\tok\\Lib\\site-packages\\kedro\\ipython\\__init__.py#149\u001b\\\u001b[2m149\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         To set a different path, run \u001b[32m'%reload_kedro \u001b[0m\u001b[32m<\u001b[0m\u001b[32mproject_root\u001b[0m\u001b[32m>\u001b[0m\u001b[32m'\u001b[0m            \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Kedro project tolkenizers                                              <a href=\"file://c:\\Users\\bcech\\anaconda3\\envs\\tok\\Lib\\site-packages\\kedro\\ipython\\__init__.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">__init__.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://c:\\Users\\bcech\\anaconda3\\envs\\tok\\Lib\\site-packages\\kedro\\ipython\\__init__.py#119\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">119</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Kedro project tolkenizers                                              \u001b]8;id=744358;file://c:\\Users\\bcech\\anaconda3\\envs\\tok\\Lib\\site-packages\\kedro\\ipython\\__init__.py\u001b\\\u001b[2m__init__.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=710089;file://c:\\Users\\bcech\\anaconda3\\envs\\tok\\Lib\\site-packages\\kedro\\ipython\\__init__.py#119\u001b\\\u001b[2m119\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Defined global variable <span style=\"color: #008000; text-decoration-color: #008000\">'context'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'session'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'catalog'</span> and            <a href=\"file://c:\\Users\\bcech\\anaconda3\\envs\\tok\\Lib\\site-packages\\kedro\\ipython\\__init__.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">__init__.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://c:\\Users\\bcech\\anaconda3\\envs\\tok\\Lib\\site-packages\\kedro\\ipython\\__init__.py#120\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">120</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">'pipelines'</span>                                                            <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Defined global variable \u001b[32m'context'\u001b[0m, \u001b[32m'session'\u001b[0m, \u001b[32m'catalog'\u001b[0m and            \u001b]8;id=598980;file://c:\\Users\\bcech\\anaconda3\\envs\\tok\\Lib\\site-packages\\kedro\\ipython\\__init__.py\u001b\\\u001b[2m__init__.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=446929;file://c:\\Users\\bcech\\anaconda3\\envs\\tok\\Lib\\site-packages\\kedro\\ipython\\__init__.py#120\u001b\\\u001b[2m120\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32m'pipelines'\u001b[0m                                                            \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Registered line magic <span style=\"color: #008000; text-decoration-color: #008000\">'run_viz'</span>                                        <a href=\"file://c:\\Users\\bcech\\anaconda3\\envs\\tok\\Lib\\site-packages\\kedro\\ipython\\__init__.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">__init__.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://c:\\Users\\bcech\\anaconda3\\envs\\tok\\Lib\\site-packages\\kedro\\ipython\\__init__.py#126\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">126</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Registered line magic \u001b[32m'run_viz'\u001b[0m                                        \u001b]8;id=285075;file://c:\\Users\\bcech\\anaconda3\\envs\\tok\\Lib\\site-packages\\kedro\\ipython\\__init__.py\u001b\\\u001b[2m__init__.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=724180;file://c:\\Users\\bcech\\anaconda3\\envs\\tok\\Lib\\site-packages\\kedro\\ipython\\__init__.py#126\u001b\\\u001b[2m126\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[02/12/24 01:07:35] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> c:\\Users\\bcech\\anaconda3\\envs\\tok\\Lib\\site-packages\\tqdm\\auto.py:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21</span>:   <a href=\"file://c:\\Users\\bcech\\anaconda3\\envs\\tok\\Lib\\warnings.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">warnings.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://c:\\Users\\bcech\\anaconda3\\envs\\tok\\Lib\\warnings.py#109\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">109</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         TqdmWarning: IProgress not found. Please update jupyter and            <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         ipywidgets. See                                                        <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://ipywidgets.readthedocs.io/en/stable/user_install.html</span>          <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>           from .autonotebook import tqdm as notebook_tqdm                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>                                                                                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[02/12/24 01:07:35]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m c:\\Users\\bcech\\anaconda3\\envs\\tok\\Lib\\site-packages\\tqdm\\auto.py:\u001b[1;36m21\u001b[0m:   \u001b]8;id=3992;file://c:\\Users\\bcech\\anaconda3\\envs\\tok\\Lib\\warnings.py\u001b\\\u001b[2mwarnings.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=648350;file://c:\\Users\\bcech\\anaconda3\\envs\\tok\\Lib\\warnings.py#109\u001b\\\u001b[2m109\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         TqdmWarning: IProgress not found. Please update jupyter and            \u001b[2m               \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         ipywidgets. See                                                        \u001b[2m               \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[4;94mhttps://ipywidgets.readthedocs.io/en/stable/user_install.html\u001b[0m          \u001b[2m               \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m           from .autonotebook import tqdm as notebook_tqdm                      \u001b[2m               \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m                                                                                \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext kedro.ipython\n",
    "%reload_kedro\n",
    "\n",
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"Stars, hide your fires; Let not light see my black and deep desires.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded = {'input_ids': [101, 3340, 1010, 5342, 2115, 8769, 1025, 2292, 2025, 2422, 2156, 2026, 2304, 1998, 2784, 14714, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "encoded = [101, 3340, 1010, 5342, 2115, 8769, 1025, 2292, 2025, 2422, 2156, 2026, 2304, 1998, 2784, 14714, 1012, 102]\n",
      "encoded_plus = {'input_ids': [101, 3340, 1010, 5342, 2115, 8769, 1025, 2292, 2025, 2422, 2156, 2026, 2304, 1998, 2784, 14714, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer(s)\n",
    "print(f\"{encoded = }\")\n",
    "encoded = tokenizer.encode(s)\n",
    "print(f\"{encoded = }\")\n",
    "encoded_plus = tokenizer.encode_plus(s)\n",
    "print(f\"{encoded_plus = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and to grab the actual tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'stars', ',', 'hide', 'your', 'fires', ';', 'let', 'not', 'light', 'see', 'my', 'black', 'and', 'deep', 'desires', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(encoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about other special characters and padding? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1999, 1996, 10250, 19440, 26077, 1998, 8670, 3489, 1025, 102, 0, 0, 0]\n",
      "[101, 3239, 1997, 25597, 1998, 11756, 1997, 10729, 102, 0, 0, 0, 0, 0]\n",
      "[101, 12121, 1997, 7151, 1998, 4416, 1997, 3899, 102, 0, 0, 0, 0, 0]\n",
      "[101, 5587, 2121, 1005, 1055, 9292, 1998, 6397, 1011, 15485, 1005, 1055, 12072, 102]\n",
      "[101, 15450, 1005, 1055, 4190, 1998, 22912, 3388, 1005, 1055, 3358, 102, 0, 0]\n",
      "[101, 2005, 1037, 11084, 1997, 3928, 4390, 102, 0, 0, 0, 0, 0, 0]\n",
      "[101, 2066, 1037, 3109, 1011, 22953, 2705, 26077, 1998, 11957, 1012, 102, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]\n",
      "['[CLS]', 'in', 'the', 'cal', '##dron', 'boil', 'and', 'ba', '##ke', ';', '[SEP]', '[PAD]', '[PAD]', '[PAD]']\n",
      "['[CLS]', 'eye', 'of', 'newt', 'and', 'toe', 'of', 'frog', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "['[CLS]', 'wool', 'of', 'bat', 'and', 'tongue', 'of', 'dog', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "['[CLS]', 'add', '##er', \"'\", 's', 'fork', 'and', 'blind', '-', 'worm', \"'\", 's', 'sting', '[SEP]']\n",
      "['[CLS]', 'lizard', \"'\", 's', 'leg', 'and', 'howl', '##et', \"'\", 's', 'wing', '[SEP]', '[PAD]', '[PAD]']\n",
      "['[CLS]', 'for', 'a', 'charm', 'of', 'powerful', 'trouble', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "['[CLS]', 'like', 'a', 'hell', '-', 'bro', '##th', 'boil', 'and', 'bubble', '.', '[SEP]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "s = [\"In the caldron boil and bake;\",\n",
    "    \"Eye of newt and toe of frog\",\n",
    "    \"Wool of bat and tongue of dog\",\n",
    "    \"Adder's fork and blind-worm's sting\",\n",
    "    \"Lizard's leg and howlet's wing\",\n",
    "    \"For a charm of powerful trouble\",\n",
    "    \"Like a hell-broth boil and bubble.\"\n",
    "]\n",
    "encoded = tokenizer(s, padding=True, add_special_tokens=True)\n",
    "\n",
    "# print ids\n",
    "for a in encoded['input_ids']:\n",
    "    print(a)\n",
    "\n",
    "# print attention mask\n",
    "for a in encoded['attention_mask']:\n",
    "    print(a)\n",
    "\n",
    "for a in encoded['input_ids']:\n",
    "    print(tokenizer.convert_ids_to_tokens(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tok",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
