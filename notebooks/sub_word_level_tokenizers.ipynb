{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sub-word-level Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall flaws of \n",
    "1. Word-based approaches\n",
    "\n",
    "* Very large vocabularies\n",
    "* Large quantity of out-of-vocabulary tokens\n",
    "* Loss of meaning across very similar words (dog, dogs, sun, sunny, hap, happy)\n",
    "\n",
    "2. Character-based approaches\n",
    "\n",
    "* Very long sequences\n",
    "* Less meaningful individual tokens\n",
    "\n",
    "So,\n",
    "\n",
    "A middle-ground between the two is found.\n",
    "\n",
    "1. Frequently used words should not be split (into subwords)\n",
    "2. Rare words should be split into \"meaningful\" subwords\n",
    "\n",
    "`dog` -> `dog`\n",
    "`dogs` -> `dog` `s`\n",
    "\n",
    "see [hf video on it](https://www.youtube.com/watch?v=zHvTiHr506c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good example given is `tokenization`: \n",
    "\n",
    "`token` -- `ization`  \n",
    "\n",
    "`token`: Token, tokens, tokenizing, tokenization, tokenized, tokenizes, tokenizable, tokenizability  \n",
    "`ization`: tokenization, modernization, globalization, industrialization, organization, realization,   utilization, ...  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sub-word-based tokenizers typically identify when a word is split, by special character padding: \n",
    "\n",
    "`token` `##ization` (BERT)\n",
    "\n",
    "The number of `#` may or may not refer to the count of preceding letters it was split from (in this case it doesn't)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sub-Word Algorithms\n",
    "\n",
    "1. WordPiece (BERT, DistilBERT,  ...)\n",
    "2. Unigram (XLNET, ALBERT)\n",
    "3. Byte-Pair Encoding (RoBERTa, GPT-2+, T5, ...)\n",
    "3. [SentencePiece](https://huggingface.co/docs/transformers/tokenizer_summary#sentencepiece+) (sorta T5, ...) ((all that use it also use Unigram))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordPiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Hugging Face Video](https://www.youtube.com/watch?v=qpv6ms_t_1A)\n",
    "* [Google Paper](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37842.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Algorithm is not open source (Google) \n",
    "\n",
    "Pseudo-algorithm : \n",
    "\n",
    "1. (similar to BPE) start with a corpus and divide each word into sequence of splits that make it up: `huggingface` -> `h` `##u` `##g` ... `##e`. Note the `##` for letters that do not start a word. \n",
    "2. Keep only one occurence per elementary unit. i.e. vocab starts `a` ...`z` + `##e` ,... (and only non-start letters that actually occurred)\n",
    "3. List all existing pairs in corpus (e.g. `h`+`##u`, `##u`+`##g`, ...) and score each pairs via: \n",
    "\n",
    "$$score=(freq_of_pair)/(freq_of_first_element√ófreq_of_second_element)$$\n",
    "\n",
    "4. Add to vocabulary pair with highest score. \n",
    "5. Add pair to splits\n",
    "6. iterate till desired size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall strategy is to start with a very large vocabulary, and iteratively shrink it. Each iteration a unigram loss is calculated and the bottom _p_ tokens are reduced.\n",
    "\n",
    "* Unigram model is stats model: $$P(t1, t2, t3, ... tN)$$\n",
    "\n",
    "* Unigram model assumes that the occurence of each word is independent of its previous word. Thus, can calulate overall probability of next word: \n",
    "\n",
    "$$P(t1, t2, t3, ... tN)= P(t1) x P(t2) x P(t3) x ... x P(TN)$$\n",
    "\n",
    "i.e. the probability of a text is the probability of the tokens that compose it. This means that it can't perform meaningful text generation: it will always predict the single highest probability token. So, what's it good for? \n",
    "\n",
    "It is a useful model to estimate the relative likelihood of different phrases. (..?)  \n",
    "\n",
    "**TODO Fill iterations**\n",
    "\n",
    "* each iteration, calculate probabilities of each token \n",
    "* remove one that impacts loss the least && is not an elementary token (as this would prevent ever spelling a word that contains it)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Byte-Pair Encoding (BPE) Tokenizaiton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea is originally text compression algorithm. \n",
    "\n",
    "Psuedo-algorithm: \n",
    "\n",
    "* Take a corpus, split into all words, then into all characters\n",
    "* count each pair frequency, finding most common pair(s)\n",
    "* take most common pair as a token and add to vocabulary\n",
    "* iterate until desired vocab size is reached "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [HuggingFace Video](https://www.youtube.com/watch?v=HEikzVL-lZU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SentencePiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext kedro.ipython\n",
    "%reload_kedro\n",
    "\n",
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"Stars, hide your fires; Let not light see my black and deep desires.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = tokenizer(s)\n",
    "print(f\"{encoded = }\")\n",
    "encoded = tokenizer.encode(s)\n",
    "print(f\"{encoded = }\")\n",
    "encoded_plus = tokenizer.encode_plus(s)\n",
    "print(f\"{encoded_plus = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and to grab the actual tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(encoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about other special characters and padding? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = [\"In the caldron boil and bake;\",\n",
    "    \"Eye of newt and toe of frog\",\n",
    "    \"Wool of bat and tongue of dog\",\n",
    "    \"Adder's fork and blind-worm's sting\",\n",
    "    \"Lizard's leg and howlet's wing\",\n",
    "    \"For a charm of powerful trouble\",\n",
    "    \"Like a hell-broth boil and bubble.\"\n",
    "]\n",
    "encoded = tokenizer(s, padding=True, add_special_tokens=True)\n",
    "\n",
    "# print ids\n",
    "for a in encoded['input_ids']:\n",
    "    print(a)\n",
    "\n",
    "# print attention mask\n",
    "for a in encoded['attention_mask']:\n",
    "    print(a)\n",
    "\n",
    "for a in encoded['input_ids']:\n",
    "    print(tokenizer.convert_ids_to_tokens(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The typical special tokens are:\n",
    "\n",
    "* [CLS] - At the beginning of the sequence\n",
    "* [SEP] - Between two sequences\n",
    "* [UNK] - When a token is unknown\n",
    "* [PAD] - At the end of the sequence\n",
    "* [MASK] - Mask (cover, hide) tokens for prediction task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tok",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
